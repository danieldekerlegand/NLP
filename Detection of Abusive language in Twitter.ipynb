{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.preprocessing import scale\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99996, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beats by Dr. Dre urBeats Wired In-Ear Headphon...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Papapishu: Man it would fucking rule if we...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is time to draw close to Him &amp;#128591;&amp;#127...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you notice me start to act different or dis...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Forget unfollowers, I believe in growing. 7 ne...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT @Vitiligoprince: Hate Being sexually Frustr...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label\n",
       "0  Beats by Dr. Dre urBeats Wired In-Ear Headphon...      2\n",
       "1  RT @Papapishu: Man it would fucking rule if we...      3\n",
       "2  It is time to draw close to Him &#128591;&#127...      1\n",
       "3  if you notice me start to act different or dis...      1\n",
       "4  Forget unfollowers, I believe in growing. 7 ne...      1\n",
       "5  RT @Vitiligoprince: Hate Being sexually Frustr...      3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the twitter dataset from the file\n",
    "data = pd.read_csv('dataset.csv',index_col=False )\n",
    "print(data.shape)\n",
    "data.head(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.tweet,data.label,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28302    so this is why your curly haircut costs that m...\n",
      "46462    seancooper but who at at dt mcdowell damn near...\n",
      "56688    bosslogic then teach me how to handle those ha...\n",
      "Name: tweet, dtype: object\n",
      "(79996,)\n",
      "1089     leadership is not just what happens when you r...\n",
      "80412    even satan will bow it says every why wait pra...\n",
      "65358    have a wonderful weekend whatever you re up to...\n",
      "Name: tweet, dtype: object\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "# Clean the text by removing ...\n",
    "\n",
    "def to_lower(word):\n",
    "    result = word.lower()\n",
    "    return result\n",
    "\n",
    "def remove_hyperlink(word):\n",
    "    return  re.sub(r\"http\\S+\", \"\", word)\n",
    "\n",
    "def remove_mentions(word):\n",
    "    return re.sub(r\"@\\S+\", \"\", word)\n",
    "\n",
    "def remove_number(word):\n",
    "    result = re.sub(r'\\d+', '', word)\n",
    "    return result\n",
    "\n",
    "def remove_punctuation(word):\n",
    "    result = re.sub('[^A-Za-z]+', ' ', word)\n",
    "    return result\n",
    "\n",
    "def remove_whitespace(word):\n",
    "    result = word.strip()\n",
    "    return result\n",
    "\n",
    "def replace_newline(word):\n",
    "    return word.replace('\\n','')\n",
    "\n",
    "def remove_stopwords(word):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    return ' '.join(word for word in i.split() if word not in stopwords)\n",
    "\n",
    "def clean_up_pipeline(sentence):\n",
    "    cleaning_data = [remove_hyperlink,\n",
    "                      replace_newline,\n",
    "                      to_lower,\n",
    "                      remove_number,\n",
    "                      remove_punctuation,\n",
    "                      remove_whitespace]\n",
    "    for func in cleaning_data:\n",
    "        \n",
    "        sentence = func(sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "X_train = X_train.apply(clean_up_pipeline)\n",
    "X_test = X_test.apply(clean_up_pipeline)\n",
    "print(X_train[:3])\n",
    "print(X_train.shape)\n",
    "print(X_test[:3])\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28302    so this is why your curly haircut costs that m...\n",
      "46462    seancooper but who at at dt mcdowell damn near...\n",
      "56688    bosslogic then teach me how to handle those ha...\n",
      "Name: tweet, dtype: object\n",
      "(79996,)\n",
      "1089     leadership is not just what happens when you r...\n",
      "80412    even satan will bow it says every why wait pra...\n",
      "65358    have a wonderful weekend whatever you re up to...\n",
      "Name: tweet, dtype: object\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# X_train = X_train.apply(word_tokenize)\n",
    "# X_test = X_test.apply(word_tokenize)\n",
    "print(X_train[:3])\n",
    "print(X_train.shape)\n",
    "print(X_test[:3])\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text])\n",
    "X_train = X_train.apply(lambda text: stem_words(text))\n",
    "X_test = X_test.apply(lambda text: stem_words(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models.word2vec import Word2Vec \n",
    "# #Word Embedding using Word2Vec\n",
    "# wordvector = Word2Vec(vector_size=300, min_count=10)\n",
    "# wordvector.build_vocab([x for x in X_train])\n",
    "# wordvector.train([x for x in X_train],total_examples=wordvector.corpus_count, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordvector.wv['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "# # Load vectors directly from the file\n",
    "# model = KeyedVectors.load_word2vec_format('data/GoogleGoogleNews-vectors-negative300.bin', binary=True)\n",
    "# # Access vectors for specific words with a keyed lookup:\n",
    "# vector = model['easy']\n",
    "# # see the shape of the vector (300,)\n",
    "# vector.shape\n",
    "# # Processing sentences is not as simple as with Spacy:\n",
    "# vectors = [model[x] for x in \"This is some text I am processing with Spacy\".split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "# path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "# print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "glove_vectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_vectors['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += glove_vectors[word].reshape((1, size)) #* tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "181it [00:00, 1802.20it/s]\u001b[A\n",
      "372it [00:00, 1861.65it/s]\u001b[A\n",
      "566it [00:00, 1895.84it/s]\u001b[A\n",
      "771it [00:00, 1954.62it/s]\u001b[A\n",
      "979it [00:00, 1998.01it/s]\u001b[A\n",
      "1179it [00:00, 1681.97it/s]\u001b[A\n",
      "1378it [00:00, 1770.92it/s]\u001b[A\n",
      "1565it [00:00, 1798.32it/s]\u001b[A\n",
      "1750it [00:00, 1808.52it/s]\u001b[A\n",
      "1949it [00:01, 1859.28it/s]\u001b[A\n",
      "2155it [00:01, 1916.58it/s]\u001b[A\n",
      "2355it [00:01, 1941.06it/s]\u001b[A\n",
      "2556it [00:01, 1961.58it/s]\u001b[A\n",
      "2762it [00:01, 1990.44it/s]\u001b[A\n",
      "2962it [00:01, 1975.37it/s]\u001b[A\n",
      "3166it [00:01, 1992.60it/s]\u001b[A\n",
      "3366it [00:01, 1991.39it/s]\u001b[A\n",
      "3567it [00:01, 1994.58it/s]\u001b[A\n",
      "3767it [00:01, 1985.19it/s]\u001b[A\n",
      "3966it [00:02, 1934.92it/s]\u001b[A\n",
      "4170it [00:02, 1964.39it/s]\u001b[A\n",
      "4373it [00:02, 1982.04it/s]\u001b[A\n",
      "4572it [00:02, 1967.29it/s]\u001b[A\n",
      "4769it [00:02, 1963.09it/s]\u001b[A\n",
      "4966it [00:02, 1957.00it/s]\u001b[A\n",
      "5162it [00:02, 1950.91it/s]\u001b[A\n",
      "5359it [00:02, 1953.69it/s]\u001b[A\n",
      "5557it [00:02, 1960.17it/s]\u001b[A\n",
      "5756it [00:02, 1967.28it/s]\u001b[A\n",
      "5953it [00:03, 1954.32it/s]\u001b[A\n",
      "6149it [00:03, 1939.69it/s]\u001b[A\n",
      "6347it [00:03, 1951.20it/s]\u001b[A\n",
      "6545it [00:03, 1956.88it/s]\u001b[A\n",
      "6747it [00:03, 1974.38it/s]\u001b[A\n",
      "6945it [00:03, 1963.08it/s]\u001b[A\n",
      "7142it [00:03, 1957.80it/s]\u001b[A\n",
      "7339it [00:03, 1960.32it/s]\u001b[A\n",
      "7536it [00:03, 1941.49it/s]\u001b[A\n",
      "7738it [00:03, 1964.42it/s]\u001b[A\n",
      "7935it [00:04, 1955.49it/s]\u001b[A\n",
      "8131it [00:04, 1865.56it/s]\u001b[A\n",
      "8319it [00:04, 1799.55it/s]\u001b[A\n",
      "8517it [00:04, 1850.44it/s]\u001b[A\n",
      "8703it [00:04, 1822.54it/s]\u001b[A\n",
      "8886it [00:04, 1771.15it/s]\u001b[A\n",
      "9091it [00:04, 1848.85it/s]\u001b[A\n",
      "9277it [00:04, 1781.85it/s]\u001b[A\n",
      "9470it [00:04, 1822.44it/s]\u001b[A\n",
      "9671it [00:05, 1876.21it/s]\u001b[A\n",
      "9860it [00:05, 1826.45it/s]\u001b[A\n",
      "10045it [00:05, 1830.62it/s]\u001b[A\n",
      "10243it [00:05, 1873.22it/s]\u001b[A\n",
      "10436it [00:05, 1886.58it/s]\u001b[A\n",
      "10626it [00:05, 1879.19it/s]\u001b[A\n",
      "10834it [00:05, 1934.85it/s]\u001b[A\n",
      "11033it [00:05, 1950.52it/s]\u001b[A\n",
      "11237it [00:05, 1976.96it/s]\u001b[A\n",
      "11443it [00:05, 2001.59it/s]\u001b[A\n",
      "11644it [00:06, 2001.25it/s]\u001b[A\n",
      "11845it [00:06, 1970.59it/s]\u001b[A\n",
      "12043it [00:06, 1969.35it/s]\u001b[A\n",
      "12244it [00:06, 1978.13it/s]\u001b[A\n",
      "12442it [00:06, 1962.51it/s]\u001b[A\n",
      "12646it [00:06, 1984.24it/s]\u001b[A\n",
      "12848it [00:06, 1993.96it/s]\u001b[A\n",
      "13048it [00:06, 1992.98it/s]\u001b[A\n",
      "13252it [00:06, 2006.09it/s]\u001b[A\n",
      "13453it [00:06, 1993.51it/s]\u001b[A\n",
      "13654it [00:07, 1997.23it/s]\u001b[A\n",
      "13854it [00:07, 1980.10it/s]\u001b[A\n",
      "14064it [00:07, 2012.98it/s]\u001b[A\n",
      "14266it [00:07, 1998.59it/s]\u001b[A\n",
      "14470it [00:07, 2009.32it/s]\u001b[A\n",
      "14671it [00:07, 1984.30it/s]\u001b[A\n",
      "14870it [00:07, 1960.81it/s]\u001b[A\n",
      "15067it [00:07, 1950.82it/s]\u001b[A\n",
      "15266it [00:07, 1959.43it/s]\u001b[A\n",
      "15462it [00:08, 1947.72it/s]\u001b[A\n",
      "15658it [00:08, 1950.22it/s]\u001b[A\n",
      "15854it [00:08, 1947.79it/s]\u001b[A\n",
      "16053it [00:08, 1958.46it/s]\u001b[A\n",
      "16255it [00:08, 1975.82it/s]\u001b[A\n",
      "16461it [00:08, 1999.48it/s]\u001b[A\n",
      "16661it [00:08, 1923.94it/s]\u001b[A\n",
      "16854it [00:08, 1889.24it/s]\u001b[A\n",
      "17044it [00:08, 1882.61it/s]\u001b[A\n",
      "17250it [00:08, 1934.12it/s]\u001b[A\n",
      "17444it [00:09, 1934.58it/s]\u001b[A\n",
      "17639it [00:09, 1936.61it/s]\u001b[A\n",
      "17841it [00:09, 1958.66it/s]\u001b[A\n",
      "18038it [00:09, 1956.10it/s]\u001b[A\n",
      "18242it [00:09, 1979.81it/s]\u001b[A\n",
      "18444it [00:09, 1991.43it/s]\u001b[A\n",
      "18644it [00:09, 1972.51it/s]\u001b[A\n",
      "18842it [00:09, 1938.86it/s]\u001b[A\n",
      "19037it [00:09, 1937.13it/s]\u001b[A\n",
      "19234it [00:09, 1945.39it/s]\u001b[A\n",
      "19429it [00:10, 1945.80it/s]\u001b[A\n",
      "19624it [00:10, 1923.77it/s]\u001b[A\n",
      "19817it [00:10, 1848.92it/s]\u001b[A\n",
      "20003it [00:10, 1773.63it/s]\u001b[A\n",
      "20205it [00:10, 1842.66it/s]\u001b[A\n",
      "20402it [00:10, 1879.25it/s]\u001b[A\n",
      "20600it [00:10, 1908.32it/s]\u001b[A\n",
      "20792it [00:10, 1908.86it/s]\u001b[A\n",
      "20989it [00:10, 1925.60it/s]\u001b[A\n",
      "21186it [00:10, 1936.99it/s]\u001b[A\n",
      "21384it [00:11, 1947.83it/s]\u001b[A\n",
      "21579it [00:11, 1878.71it/s]\u001b[A\n",
      "21776it [00:11, 1905.24it/s]\u001b[A\n",
      "21978it [00:11, 1936.90it/s]\u001b[A\n",
      "22180it [00:11, 1957.68it/s]\u001b[A\n",
      "22377it [00:11, 1949.81it/s]\u001b[A\n",
      "22577it [00:11, 1963.66it/s]\u001b[A\n",
      "22774it [00:11, 1957.44it/s]\u001b[A\n",
      "22973it [00:11, 1965.21it/s]\u001b[A\n",
      "23171it [00:11, 1966.85it/s]\u001b[A\n",
      "23369it [00:12, 1967.61it/s]\u001b[A\n",
      "23566it [00:12, 1787.62it/s]\u001b[A\n",
      "23748it [00:12, 1772.90it/s]\u001b[A\n",
      "23928it [00:12, 1744.25it/s]\u001b[A\n",
      "24104it [00:12, 1723.49it/s]\u001b[A\n",
      "24282it [00:12, 1737.28it/s]\u001b[A\n",
      "24475it [00:12, 1790.90it/s]\u001b[A\n",
      "24667it [00:12, 1828.26it/s]\u001b[A\n",
      "24852it [00:12, 1833.64it/s]\u001b[A\n",
      "25046it [00:13, 1864.07it/s]\u001b[A\n",
      "25247it [00:13, 1904.05it/s]\u001b[A\n",
      "25448it [00:13, 1931.98it/s]\u001b[A\n",
      "25645it [00:13, 1942.36it/s]\u001b[A\n",
      "25840it [00:13, 1888.10it/s]\u001b[A\n",
      "26036it [00:13, 1907.08it/s]\u001b[A\n",
      "26239it [00:13, 1942.65it/s]\u001b[A\n",
      "26439it [00:13, 1957.58it/s]\u001b[A\n",
      "26645it [00:13, 1985.86it/s]\u001b[A\n",
      "26852it [00:13, 2008.76it/s]\u001b[A\n",
      "27054it [00:14, 1999.04it/s]\u001b[A\n",
      "27255it [00:14, 1997.93it/s]\u001b[A\n",
      "27455it [00:14, 1941.76it/s]\u001b[A\n",
      "27652it [00:14, 1947.26it/s]\u001b[A\n",
      "27847it [00:14, 1944.34it/s]\u001b[A\n",
      "28051it [00:14, 1971.07it/s]\u001b[A\n",
      "28254it [00:14, 1987.80it/s]\u001b[A\n",
      "28453it [00:14, 1974.96it/s]\u001b[A\n",
      "28651it [00:14, 1951.35it/s]\u001b[A\n",
      "28850it [00:14, 1959.15it/s]\u001b[A\n",
      "29051it [00:15, 1973.36it/s]\u001b[A\n",
      "29252it [00:15, 1982.40it/s]\u001b[A\n",
      "29451it [00:15, 1949.17it/s]\u001b[A\n",
      "29647it [00:15, 1944.53it/s]\u001b[A\n",
      "29854it [00:15, 1980.80it/s]\u001b[A\n",
      "30053it [00:15, 1978.92it/s]\u001b[A\n",
      "30251it [00:15, 1963.68it/s]\u001b[A\n",
      "30448it [00:15, 1964.45it/s]\u001b[A\n",
      "30645it [00:15, 1957.84it/s]\u001b[A\n",
      "30841it [00:16, 1936.45it/s]\u001b[A\n",
      "31035it [00:16, 1936.62it/s]\u001b[A\n",
      "31229it [00:16, 1878.42it/s]\u001b[A\n",
      "31433it [00:16, 1924.42it/s]\u001b[A\n",
      "31626it [00:16, 1879.09it/s]\u001b[A\n",
      "31815it [00:16, 1881.29it/s]\u001b[A\n",
      "32004it [00:16, 1829.10it/s]\u001b[A\n",
      "32199it [00:16, 1862.47it/s]\u001b[A\n",
      "32398it [00:16, 1899.13it/s]\u001b[A\n",
      "32596it [00:16, 1921.18it/s]\u001b[A\n",
      "32801it [00:17, 1957.89it/s]\u001b[A\n",
      "33005it [00:17, 1980.96it/s]\u001b[A\n",
      "33206it [00:17, 1987.55it/s]\u001b[A\n",
      "33408it [00:17, 1996.00it/s]\u001b[A\n",
      "33609it [00:17, 1998.00it/s]\u001b[A\n",
      "33809it [00:17, 1991.24it/s]\u001b[A\n",
      "34009it [00:17, 1986.84it/s]\u001b[A\n",
      "34219it [00:17, 2018.45it/s]\u001b[A\n",
      "34421it [00:17, 2004.10it/s]\u001b[A\n",
      "34622it [00:17, 1952.73it/s]\u001b[A\n",
      "34825it [00:18, 1972.09it/s]\u001b[A\n",
      "35024it [00:18, 1975.57it/s]\u001b[A\n",
      "35224it [00:18, 1981.01it/s]\u001b[A\n",
      "35423it [00:18, 1972.17it/s]\u001b[A\n",
      "35621it [00:18, 1961.50it/s]\u001b[A\n",
      "35818it [00:18, 1963.62it/s]\u001b[A\n",
      "36015it [00:18, 1964.28it/s]\u001b[A\n",
      "36212it [00:18, 1962.97it/s]\u001b[A\n",
      "36409it [00:18, 1928.78it/s]\u001b[A\n",
      "36603it [00:18, 1879.97it/s]\u001b[A\n",
      "36807it [00:19, 1924.61it/s]\u001b[A\n",
      "37000it [00:19, 1891.74it/s]\u001b[A\n",
      "37193it [00:19, 1900.77it/s]\u001b[A\n",
      "37392it [00:19, 1926.84it/s]\u001b[A\n",
      "37587it [00:19, 1931.60it/s]\u001b[A\n",
      "37781it [00:19, 1897.27it/s]\u001b[A\n",
      "37977it [00:19, 1913.56it/s]\u001b[A\n",
      "38173it [00:19, 1924.99it/s]\u001b[A\n",
      "38368it [00:19, 1931.31it/s]\u001b[A\n",
      "38562it [00:19, 1929.63it/s]\u001b[A\n",
      "38756it [00:20, 1932.40it/s]\u001b[A\n",
      "38950it [00:20, 1931.95it/s]\u001b[A\n",
      "39155it [00:20, 1966.24it/s]\u001b[A\n",
      "39352it [00:20, 1957.19it/s]\u001b[A\n",
      "39548it [00:20, 1956.30it/s]\u001b[A\n",
      "39749it [00:20, 1970.07it/s]\u001b[A\n",
      "39947it [00:20, 1950.18it/s]\u001b[A\n",
      "40147it [00:20, 1963.82it/s]\u001b[A\n",
      "40349it [00:20, 1980.19it/s]\u001b[A\n",
      "40551it [00:20, 1991.05it/s]\u001b[A\n",
      "40751it [00:21, 1977.23it/s]\u001b[A\n",
      "40958it [00:21, 2003.81it/s]\u001b[A\n",
      "41162it [00:21, 2013.06it/s]\u001b[A\n",
      "41364it [00:21, 2008.86it/s]\u001b[A\n",
      "41565it [00:21, 1984.86it/s]\u001b[A\n",
      "41766it [00:21, 1991.90it/s]\u001b[A\n",
      "41966it [00:21, 1979.30it/s]\u001b[A\n",
      "42164it [00:21, 1900.36it/s]\u001b[A\n",
      "42359it [00:21, 1914.53it/s]\u001b[A\n",
      "42560it [00:22, 1940.15it/s]\u001b[A\n",
      "42755it [00:22, 1929.54it/s]\u001b[A\n",
      "42949it [00:22, 1925.21it/s]\u001b[A\n",
      "43149it [00:22, 1946.38it/s]\u001b[A\n",
      "43344it [00:22, 1927.25it/s]\u001b[A\n",
      "43548it [00:22, 1959.31it/s]\u001b[A\n",
      "43745it [00:22, 1926.91it/s]\u001b[A\n",
      "43938it [00:22, 1900.41it/s]\u001b[A\n",
      "44133it [00:22, 1911.83it/s]\u001b[A\n",
      "44342it [00:22, 1961.93it/s]\u001b[A\n",
      "44539it [00:23, 1955.82it/s]\u001b[A\n",
      "44735it [00:23, 1941.99it/s]\u001b[A\n",
      "44930it [00:23, 1850.82it/s]\u001b[A\n",
      "45116it [00:23, 1815.08it/s]\u001b[A\n",
      "45299it [00:23, 1750.28it/s]\u001b[A\n",
      "45475it [00:23, 1737.47it/s]\u001b[A\n",
      "45654it [00:23, 1749.16it/s]\u001b[A\n",
      "45830it [00:23, 1736.12it/s]\u001b[A\n",
      "46004it [00:23, 1727.47it/s]\u001b[A\n",
      "46178it [00:23, 1730.38it/s]\u001b[A\n",
      "46352it [00:24, 1709.62it/s]\u001b[A\n",
      "46548it [00:24, 1781.28it/s]\u001b[A\n",
      "46737it [00:24, 1813.19it/s]\u001b[A\n",
      "46934it [00:24, 1858.44it/s]\u001b[A\n",
      "47131it [00:24, 1889.30it/s]\u001b[A\n",
      "47335it [00:24, 1934.17it/s]\u001b[A\n",
      "47534it [00:24, 1949.18it/s]\u001b[A\n",
      "47734it [00:24, 1963.86it/s]\u001b[A\n",
      "47931it [00:24, 1941.47it/s]\u001b[A\n",
      "48135it [00:24, 1969.76it/s]\u001b[A\n",
      "48333it [00:25, 1958.21it/s]\u001b[A\n",
      "48541it [00:25, 1993.86it/s]\u001b[A\n",
      "48741it [00:25, 1993.07it/s]\u001b[A\n",
      "48941it [00:25, 1939.91it/s]\u001b[A\n",
      "49136it [00:25, 1940.39it/s]\u001b[A\n",
      "49331it [00:25, 1934.85it/s]\u001b[A\n",
      "49527it [00:25, 1941.54it/s]\u001b[A\n",
      "49734it [00:25, 1977.12it/s]\u001b[A\n",
      "49932it [00:25, 1976.02it/s]\u001b[A\n",
      "50139it [00:26, 2003.11it/s]\u001b[A\n",
      "50340it [00:26, 1932.00it/s]\u001b[A\n",
      "50539it [00:26, 1948.24it/s]\u001b[A\n",
      "50735it [00:26, 1934.95it/s]\u001b[A\n",
      "50932it [00:26, 1944.04it/s]\u001b[A\n",
      "51127it [00:26, 1922.29it/s]\u001b[A\n",
      "51323it [00:26, 1931.72it/s]\u001b[A\n",
      "51523it [00:26, 1949.45it/s]\u001b[A\n",
      "51719it [00:26, 1944.33it/s]\u001b[A\n",
      "51914it [00:26, 1938.31it/s]\u001b[A\n",
      "52111it [00:27, 1946.32it/s]\u001b[A\n",
      "52306it [00:27, 1934.50it/s]\u001b[A\n",
      "52500it [00:27, 1931.57it/s]\u001b[A\n",
      "52695it [00:27, 1934.05it/s]\u001b[A\n",
      "52889it [00:27, 1912.75it/s]\u001b[A\n",
      "53087it [00:27, 1930.65it/s]\u001b[A\n",
      "53289it [00:27, 1953.98it/s]\u001b[A\n",
      "53486it [00:27, 1957.06it/s]\u001b[A\n",
      "53684it [00:27, 1962.52it/s]\u001b[A\n",
      "53881it [00:27, 1962.46it/s]\u001b[A\n",
      "54081it [00:28, 1972.34it/s]\u001b[A\n",
      "54279it [00:28, 1970.43it/s]\u001b[A\n",
      "54479it [00:28, 1976.46it/s]\u001b[A\n",
      "54677it [00:28, 1965.90it/s]\u001b[A\n",
      "54874it [00:28, 1834.80it/s]\u001b[A\n",
      "55062it [00:28, 1844.55it/s]\u001b[A\n",
      "55260it [00:28, 1882.58it/s]\u001b[A\n",
      "55458it [00:28, 1909.48it/s]\u001b[A\n",
      "55651it [00:28, 1915.09it/s]\u001b[A\n",
      "55844it [00:28, 1904.68it/s]\u001b[A\n",
      "56038it [00:29, 1914.11it/s]\u001b[A\n",
      "56231it [00:29, 1916.79it/s]\u001b[A\n",
      "56429it [00:29, 1934.91it/s]\u001b[A\n",
      "56635it [00:29, 1969.57it/s]\u001b[A\n",
      "56833it [00:29, 1971.34it/s]\u001b[A\n",
      "57031it [00:29, 1962.02it/s]\u001b[A\n",
      "57228it [00:29, 1944.45it/s]\u001b[A\n",
      "57424it [00:29, 1947.63it/s]\u001b[A\n",
      "57619it [00:29, 1913.22it/s]\u001b[A\n",
      "57811it [00:29, 1914.71it/s]\u001b[A\n",
      "58003it [00:30, 1863.05it/s]\u001b[A\n",
      "58200it [00:30, 1891.84it/s]\u001b[A\n",
      "58393it [00:30, 1899.98it/s]\u001b[A\n",
      "58584it [00:30, 1896.69it/s]\u001b[A\n",
      "58774it [00:30, 1887.99it/s]\u001b[A\n",
      "58967it [00:30, 1897.80it/s]\u001b[A\n",
      "59165it [00:30, 1922.19it/s]\u001b[A\n",
      "59363it [00:30, 1939.25it/s]\u001b[A\n",
      "59558it [00:30, 1942.00it/s]\u001b[A\n",
      "59760it [00:31, 1964.40it/s]\u001b[A\n",
      "59961it [00:31, 1977.53it/s]\u001b[A\n",
      "60162it [00:31, 1984.42it/s]\u001b[A\n",
      "60367it [00:31, 2003.62it/s]\u001b[A\n",
      "60568it [00:31, 1993.43it/s]\u001b[A\n",
      "60770it [00:31, 1998.90it/s]\u001b[A\n",
      "60973it [00:31, 2006.85it/s]\u001b[A\n",
      "61174it [00:31, 1981.20it/s]\u001b[A\n",
      "61377it [00:31, 1995.00it/s]\u001b[A\n",
      "61578it [00:31, 1997.25it/s]\u001b[A\n",
      "61778it [00:32, 1978.65it/s]\u001b[A\n",
      "61978it [00:32, 1982.51it/s]\u001b[A\n",
      "62178it [00:32, 1986.85it/s]\u001b[A\n",
      "62377it [00:32, 1977.73it/s]\u001b[A\n",
      "62575it [00:32, 1958.84it/s]\u001b[A\n",
      "62771it [00:32, 1955.82it/s]\u001b[A\n",
      "62968it [00:32, 1957.57it/s]\u001b[A\n",
      "63168it [00:32, 1969.58it/s]\u001b[A\n",
      "63366it [00:32, 1971.29it/s]\u001b[A\n",
      "63564it [00:32, 1971.90it/s]\u001b[A\n",
      "63771it [00:33, 1997.98it/s]\u001b[A\n",
      "63976it [00:33, 2012.61it/s]\u001b[A\n",
      "64178it [00:33, 1991.23it/s]\u001b[A\n",
      "64378it [00:33, 1984.88it/s]\u001b[A\n",
      "64577it [00:33, 1964.90it/s]\u001b[A\n",
      "64775it [00:33, 1966.71it/s]\u001b[A\n",
      "64972it [00:33, 1949.37it/s]\u001b[A\n",
      "65174it [00:33, 1969.28it/s]\u001b[A\n",
      "65374it [00:33, 1976.16it/s]\u001b[A\n",
      "65572it [00:33, 1975.46it/s]\u001b[A\n",
      "65770it [00:34, 1965.80it/s]\u001b[A\n",
      "65967it [00:34, 1933.72it/s]\u001b[A\n",
      "66161it [00:34, 1928.72it/s]\u001b[A\n",
      "66364it [00:34, 1956.31it/s]\u001b[A\n",
      "66560it [00:34, 1941.14it/s]\u001b[A\n",
      "66755it [00:34, 1941.44it/s]\u001b[A\n",
      "66953it [00:34, 1952.86it/s]\u001b[A\n",
      "67149it [00:34, 1944.31it/s]\u001b[A\n",
      "67354it [00:34, 1973.75it/s]\u001b[A\n",
      "67552it [00:34, 1957.16it/s]\u001b[A\n",
      "67758it [00:35, 1987.09it/s]\u001b[A\n",
      "67957it [00:35, 1986.81it/s]\u001b[A\n",
      "68157it [00:35, 1988.58it/s]\u001b[A\n",
      "68359it [00:35, 1997.81it/s]\u001b[A\n",
      "68559it [00:35, 1987.36it/s]\u001b[A\n",
      "68765it [00:35, 2005.94it/s]\u001b[A\n",
      "68966it [00:35, 1913.54it/s]\u001b[A\n",
      "69159it [00:35, 1915.69it/s]\u001b[A\n",
      "69360it [00:35, 1941.70it/s]\u001b[A\n",
      "69556it [00:35, 1944.75it/s]\u001b[A\n",
      "69759it [00:36, 1968.05it/s]\u001b[A\n",
      "69962it [00:36, 1985.98it/s]\u001b[A\n",
      "70161it [00:36, 1986.13it/s]\u001b[A\n",
      "70360it [00:36, 1986.72it/s]\u001b[A\n",
      "70559it [00:36, 1983.64it/s]\u001b[A\n",
      "70758it [00:36, 1979.59it/s]\u001b[A\n",
      "70957it [00:36, 1933.42it/s]\u001b[A\n",
      "71151it [00:36, 1847.15it/s]\u001b[A\n",
      "71337it [00:36, 1843.20it/s]\u001b[A\n",
      "71540it [00:37, 1896.24it/s]\u001b[A\n",
      "71736it [00:37, 1913.00it/s]\u001b[A\n",
      "71936it [00:37, 1937.41it/s]\u001b[A\n",
      "72134it [00:37, 1949.10it/s]\u001b[A\n",
      "72333it [00:37, 1961.12it/s]\u001b[A\n",
      "72530it [00:37, 1940.68it/s]\u001b[A\n",
      "72726it [00:37, 1943.89it/s]\u001b[A\n",
      "72924it [00:37, 1952.33it/s]\u001b[A\n",
      "73120it [00:37, 1919.07it/s]\u001b[A\n",
      "73313it [00:37, 1750.02it/s]\u001b[A\n",
      "73491it [00:38, 1729.89it/s]\u001b[A\n",
      "73666it [00:38, 1728.40it/s]\u001b[A\n",
      "73841it [00:38, 1718.39it/s]\u001b[A\n",
      "74022it [00:38, 1740.55it/s]\u001b[A\n",
      "74210it [00:38, 1778.88it/s]\u001b[A\n",
      "74406it [00:38, 1830.48it/s]\u001b[A\n",
      "74606it [00:38, 1880.24it/s]\u001b[A\n",
      "74795it [00:38, 1881.60it/s]\u001b[A\n",
      "74992it [00:38, 1905.29it/s]\u001b[A\n",
      "75191it [00:38, 1928.39it/s]\u001b[A\n",
      "75385it [00:39, 1914.53it/s]\u001b[A\n",
      "75577it [00:39, 1906.68it/s]\u001b[A\n",
      "75768it [00:39, 1895.80it/s]\u001b[A\n",
      "75971it [00:39, 1932.88it/s]\u001b[A\n",
      "76165it [00:39, 1858.64it/s]\u001b[A\n",
      "76370it [00:39, 1911.39it/s]\u001b[A\n",
      "76565it [00:39, 1921.97it/s]\u001b[A\n",
      "76760it [00:39, 1928.22it/s]\u001b[A\n",
      "76958it [00:39, 1940.58it/s]\u001b[A\n",
      "77153it [00:39, 1936.19it/s]\u001b[A\n",
      "77357it [00:40, 1964.30it/s]\u001b[A\n",
      "77557it [00:40, 1973.31it/s]\u001b[A\n",
      "77756it [00:40, 1976.37it/s]\u001b[A\n",
      "77962it [00:40, 2000.20it/s]\u001b[A\n",
      "78163it [00:40, 1997.35it/s]\u001b[A\n",
      "78363it [00:40, 1974.23it/s]\u001b[A\n",
      "78561it [00:40, 1970.26it/s]\u001b[A\n",
      "78759it [00:40, 1973.14it/s]\u001b[A\n",
      "78967it [00:40, 2003.01it/s]\u001b[A\n",
      "79170it [00:40, 2010.64it/s]\u001b[A\n",
      "79372it [00:41, 1992.02it/s]\u001b[A\n",
      "79572it [00:41, 1970.71it/s]\u001b[A\n",
      "79773it [00:41, 1980.76it/s]\u001b[A\n",
      "79996it [00:41, 1931.60it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, 300) for z in tqdm(map(lambda x: x, X_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, 300) for z in map(lambda x: x, X_test)])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28302    1\n",
      "46462    1\n",
      "56688    3\n",
      "56418    1\n",
      "2229     1\n",
      "        ..\n",
      "42798    1\n",
      "4762     1\n",
      "21434    1\n",
      "52953    1\n",
      "6960     1\n",
      "Name: label, Length: 79996, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [11:54, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train)\n",
    "from keras.utils import to_categorical\n",
    "encoded = to_categorical(y_train)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "2500/2500 - 5s - loss: 0.9541 - accuracy: 0.6403\n",
      "Epoch 2/300\n",
      "2500/2500 - 5s - loss: 0.9122 - accuracy: 0.6551\n",
      "Epoch 3/300\n",
      "2500/2500 - 5s - loss: 0.9019 - accuracy: 0.6568\n",
      "Epoch 4/300\n",
      "2500/2500 - 5s - loss: 0.8977 - accuracy: 0.6589\n",
      "Epoch 5/300\n",
      "2500/2500 - 5s - loss: 0.8942 - accuracy: 0.6605\n",
      "Epoch 6/300\n",
      "2500/2500 - 5s - loss: 0.8905 - accuracy: 0.6605\n",
      "Epoch 7/300\n",
      "2500/2500 - 5s - loss: 0.8881 - accuracy: 0.6620\n",
      "Epoch 8/300\n",
      "2500/2500 - 5s - loss: 0.8863 - accuracy: 0.6624\n",
      "Epoch 9/300\n",
      "2500/2500 - 5s - loss: 0.8839 - accuracy: 0.6634\n",
      "Epoch 10/300\n",
      "2500/2500 - 5s - loss: 0.8824 - accuracy: 0.6643\n",
      "Epoch 11/300\n",
      "2500/2500 - 5s - loss: 0.8808 - accuracy: 0.6648\n",
      "Epoch 12/300\n",
      "2500/2500 - 5s - loss: 0.8800 - accuracy: 0.6658\n",
      "Epoch 13/300\n",
      "2500/2500 - 5s - loss: 0.8792 - accuracy: 0.6666\n",
      "Epoch 14/300\n",
      "2500/2500 - 5s - loss: 0.8779 - accuracy: 0.6663\n",
      "Epoch 15/300\n",
      "2500/2500 - 5s - loss: 0.8773 - accuracy: 0.6669\n",
      "Epoch 16/300\n",
      "2500/2500 - 5s - loss: 0.8761 - accuracy: 0.6660\n",
      "Epoch 17/300\n",
      "2500/2500 - 5s - loss: 0.8759 - accuracy: 0.6679\n",
      "Epoch 18/300\n",
      "2500/2500 - 5s - loss: 0.8749 - accuracy: 0.6688\n",
      "Epoch 19/300\n",
      "2500/2500 - 5s - loss: 0.8737 - accuracy: 0.6692\n",
      "Epoch 20/300\n",
      "2500/2500 - 5s - loss: 0.8732 - accuracy: 0.6690\n",
      "Epoch 21/300\n",
      "2500/2500 - 5s - loss: 0.8720 - accuracy: 0.6719\n",
      "Epoch 22/300\n",
      "2500/2500 - 5s - loss: 0.8722 - accuracy: 0.6691\n",
      "Epoch 23/300\n",
      "2500/2500 - 5s - loss: 0.8703 - accuracy: 0.6710\n",
      "Epoch 24/300\n",
      "2500/2500 - 5s - loss: 0.8705 - accuracy: 0.6708\n",
      "Epoch 25/300\n",
      "2500/2500 - 5s - loss: 0.8693 - accuracy: 0.6708\n",
      "Epoch 26/300\n",
      "2500/2500 - 5s - loss: 0.8695 - accuracy: 0.6712\n",
      "Epoch 27/300\n",
      "2500/2500 - 5s - loss: 0.8677 - accuracy: 0.6718\n",
      "Epoch 28/300\n",
      "2500/2500 - 5s - loss: 0.8686 - accuracy: 0.6710\n",
      "Epoch 29/300\n",
      "2500/2500 - 5s - loss: 0.8677 - accuracy: 0.6716\n",
      "Epoch 30/300\n",
      "2500/2500 - 5s - loss: 0.8671 - accuracy: 0.6717\n",
      "Epoch 31/300\n",
      "2500/2500 - 5s - loss: 0.8670 - accuracy: 0.6729\n",
      "Epoch 32/300\n",
      "2500/2500 - 5s - loss: 0.8655 - accuracy: 0.6730\n",
      "Epoch 33/300\n",
      "2500/2500 - 5s - loss: 0.8648 - accuracy: 0.6733\n",
      "Epoch 34/300\n",
      "2500/2500 - 5s - loss: 0.8656 - accuracy: 0.6722\n",
      "Epoch 35/300\n",
      "2500/2500 - 5s - loss: 0.8648 - accuracy: 0.6727\n",
      "Epoch 36/300\n",
      "2500/2500 - 5s - loss: 0.8637 - accuracy: 0.6738\n",
      "Epoch 37/300\n",
      "2500/2500 - 5s - loss: 0.8633 - accuracy: 0.6733\n",
      "Epoch 38/300\n",
      "2500/2500 - 5s - loss: 0.8617 - accuracy: 0.6751\n",
      "Epoch 39/300\n",
      "2500/2500 - 5s - loss: 0.8627 - accuracy: 0.6740\n",
      "Epoch 40/300\n",
      "2500/2500 - 5s - loss: 0.8611 - accuracy: 0.6735\n",
      "Epoch 41/300\n",
      "2500/2500 - 5s - loss: 0.8611 - accuracy: 0.6739\n",
      "Epoch 42/300\n",
      "2500/2500 - 5s - loss: 0.8595 - accuracy: 0.6737\n",
      "Epoch 43/300\n",
      "2500/2500 - 5s - loss: 0.8595 - accuracy: 0.6761\n",
      "Epoch 44/300\n",
      "2500/2500 - 5s - loss: 0.8606 - accuracy: 0.6741\n",
      "Epoch 45/300\n",
      "2500/2500 - 5s - loss: 0.8582 - accuracy: 0.6757\n",
      "Epoch 46/300\n",
      "2500/2500 - 5s - loss: 0.8594 - accuracy: 0.6750\n",
      "Epoch 47/300\n",
      "2500/2500 - 5s - loss: 0.8584 - accuracy: 0.6748\n",
      "Epoch 48/300\n",
      "2500/2500 - 5s - loss: 0.8572 - accuracy: 0.6760\n",
      "Epoch 49/300\n",
      "2500/2500 - 5s - loss: 0.8581 - accuracy: 0.6755\n",
      "Epoch 50/300\n",
      "2500/2500 - 5s - loss: 0.8575 - accuracy: 0.6744\n",
      "Epoch 51/300\n",
      "2500/2500 - 5s - loss: 0.8573 - accuracy: 0.6762\n",
      "Epoch 52/300\n",
      "2500/2500 - 5s - loss: 0.8567 - accuracy: 0.6764\n",
      "Epoch 53/300\n",
      "2500/2500 - 5s - loss: 0.8559 - accuracy: 0.6762\n",
      "Epoch 54/300\n",
      "2500/2500 - 5s - loss: 0.8570 - accuracy: 0.6757\n",
      "Epoch 55/300\n",
      "2500/2500 - 5s - loss: 0.8556 - accuracy: 0.6769\n",
      "Epoch 56/300\n",
      "2500/2500 - 5s - loss: 0.8553 - accuracy: 0.6765\n",
      "Epoch 57/300\n",
      "2500/2500 - 5s - loss: 0.8550 - accuracy: 0.6762\n",
      "Epoch 58/300\n",
      "2500/2500 - 5s - loss: 0.8558 - accuracy: 0.6755\n",
      "Epoch 59/300\n",
      "2500/2500 - 5s - loss: 0.8539 - accuracy: 0.6761\n",
      "Epoch 60/300\n",
      "2500/2500 - 5s - loss: 0.8542 - accuracy: 0.6765\n",
      "Epoch 61/300\n",
      "2500/2500 - 5s - loss: 0.8555 - accuracy: 0.6780\n",
      "Epoch 62/300\n",
      "2500/2500 - 5s - loss: 0.8541 - accuracy: 0.6763\n",
      "Epoch 63/300\n",
      "2500/2500 - 5s - loss: 0.8538 - accuracy: 0.6780\n",
      "Epoch 64/300\n",
      "2500/2500 - 5s - loss: 0.8521 - accuracy: 0.6785\n",
      "Epoch 65/300\n",
      "2500/2500 - 5s - loss: 0.8522 - accuracy: 0.6779\n",
      "Epoch 66/300\n",
      "2500/2500 - 5s - loss: 0.8523 - accuracy: 0.6779\n",
      "Epoch 67/300\n",
      "2500/2500 - 5s - loss: 0.8519 - accuracy: 0.6773\n",
      "Epoch 68/300\n",
      "2500/2500 - 5s - loss: 0.8508 - accuracy: 0.6785\n",
      "Epoch 69/300\n",
      "2500/2500 - 5s - loss: 0.8508 - accuracy: 0.6778\n",
      "Epoch 70/300\n",
      "2500/2500 - 5s - loss: 0.8514 - accuracy: 0.6787\n",
      "Epoch 71/300\n",
      "2500/2500 - 5s - loss: 0.8513 - accuracy: 0.6771\n",
      "Epoch 72/300\n",
      "2500/2500 - 5s - loss: 0.8488 - accuracy: 0.6795\n",
      "Epoch 73/300\n",
      "2500/2500 - 5s - loss: 0.8491 - accuracy: 0.6783\n",
      "Epoch 74/300\n",
      "2500/2500 - 5s - loss: 0.8501 - accuracy: 0.6784\n",
      "Epoch 75/300\n",
      "2500/2500 - 5s - loss: 0.8505 - accuracy: 0.6791\n",
      "Epoch 76/300\n",
      "2500/2500 - 5s - loss: 0.8489 - accuracy: 0.6784\n",
      "Epoch 77/300\n",
      "2500/2500 - 5s - loss: 0.8494 - accuracy: 0.6785\n",
      "Epoch 78/300\n",
      "2500/2500 - 5s - loss: 0.8477 - accuracy: 0.6793\n",
      "Epoch 79/300\n",
      "2500/2500 - 5s - loss: 0.8494 - accuracy: 0.6799\n",
      "Epoch 80/300\n",
      "2500/2500 - 5s - loss: 0.8480 - accuracy: 0.6803\n",
      "Epoch 81/300\n",
      "2500/2500 - 5s - loss: 0.8498 - accuracy: 0.6801\n",
      "Epoch 82/300\n",
      "2500/2500 - 5s - loss: 0.8464 - accuracy: 0.6799\n",
      "Epoch 83/300\n",
      "2500/2500 - 5s - loss: 0.8497 - accuracy: 0.6799\n",
      "Epoch 84/300\n",
      "2500/2500 - 5s - loss: 0.8469 - accuracy: 0.6796\n",
      "Epoch 85/300\n",
      "2500/2500 - 5s - loss: 0.8489 - accuracy: 0.6796\n",
      "Epoch 86/300\n",
      "2500/2500 - 5s - loss: 0.8463 - accuracy: 0.6804\n",
      "Epoch 87/300\n",
      "2500/2500 - 5s - loss: 0.8466 - accuracy: 0.6803\n",
      "Epoch 88/300\n",
      "2500/2500 - 5s - loss: 0.8471 - accuracy: 0.6805\n",
      "Epoch 89/300\n",
      "2500/2500 - 5s - loss: 0.8463 - accuracy: 0.6799\n",
      "Epoch 90/300\n",
      "2500/2500 - 5s - loss: 0.8462 - accuracy: 0.6798\n",
      "Epoch 91/300\n",
      "2500/2500 - 5s - loss: 0.8457 - accuracy: 0.6805\n",
      "Epoch 92/300\n",
      "2500/2500 - 5s - loss: 0.8458 - accuracy: 0.6802\n",
      "Epoch 93/300\n",
      "2500/2500 - 5s - loss: 0.8461 - accuracy: 0.6797\n",
      "Epoch 94/300\n",
      "2500/2500 - 5s - loss: 0.8459 - accuracy: 0.6800\n",
      "Epoch 95/300\n",
      "2500/2500 - 5s - loss: 0.8459 - accuracy: 0.6817\n",
      "Epoch 96/300\n",
      "2500/2500 - 5s - loss: 0.8456 - accuracy: 0.6812\n",
      "Epoch 97/300\n",
      "2500/2500 - 5s - loss: 0.8430 - accuracy: 0.6819\n",
      "Epoch 98/300\n",
      "2500/2500 - 5s - loss: 0.8438 - accuracy: 0.6813\n",
      "Epoch 99/300\n",
      "2500/2500 - 5s - loss: 0.8436 - accuracy: 0.6811\n",
      "Epoch 100/300\n",
      "2500/2500 - 5s - loss: 0.8428 - accuracy: 0.6808\n",
      "Epoch 101/300\n",
      "2500/2500 - 5s - loss: 0.8453 - accuracy: 0.6811\n",
      "Epoch 102/300\n",
      "2500/2500 - 5s - loss: 0.8425 - accuracy: 0.6823\n",
      "Epoch 103/300\n",
      "2500/2500 - 5s - loss: 0.8436 - accuracy: 0.6815\n",
      "Epoch 104/300\n",
      "2500/2500 - 5s - loss: 0.8423 - accuracy: 0.6824\n",
      "Epoch 105/300\n",
      "2500/2500 - 5s - loss: 0.8448 - accuracy: 0.6816\n",
      "Epoch 106/300\n",
      "2500/2500 - 5s - loss: 0.8425 - accuracy: 0.6825\n",
      "Epoch 107/300\n",
      "2500/2500 - 5s - loss: 0.8425 - accuracy: 0.6820\n",
      "Epoch 108/300\n",
      "2500/2500 - 5s - loss: 0.8429 - accuracy: 0.6828\n",
      "Epoch 109/300\n",
      "2500/2500 - 5s - loss: 0.8428 - accuracy: 0.6825\n",
      "Epoch 110/300\n",
      "2500/2500 - 5s - loss: 0.8416 - accuracy: 0.6819\n",
      "Epoch 111/300\n",
      "2500/2500 - 5s - loss: 0.8420 - accuracy: 0.6820\n",
      "Epoch 112/300\n",
      "2500/2500 - 5s - loss: 0.8410 - accuracy: 0.6830\n",
      "Epoch 113/300\n",
      "2500/2500 - 5s - loss: 0.8418 - accuracy: 0.6819\n",
      "Epoch 114/300\n",
      "2500/2500 - 5s - loss: 0.8417 - accuracy: 0.6820\n",
      "Epoch 115/300\n",
      "2500/2500 - 5s - loss: 0.8419 - accuracy: 0.6813\n",
      "Epoch 116/300\n",
      "2500/2500 - 5s - loss: 0.8385 - accuracy: 0.6831\n",
      "Epoch 117/300\n",
      "2500/2500 - 5s - loss: 0.8406 - accuracy: 0.6833\n",
      "Epoch 118/300\n",
      "2500/2500 - 5s - loss: 0.8407 - accuracy: 0.6826\n",
      "Epoch 119/300\n",
      "2500/2500 - 5s - loss: 0.8426 - accuracy: 0.6820\n",
      "Epoch 120/300\n",
      "2500/2500 - 5s - loss: 0.8381 - accuracy: 0.6838\n",
      "Epoch 121/300\n",
      "2500/2500 - 5s - loss: 0.8413 - accuracy: 0.6827\n",
      "Epoch 122/300\n",
      "2500/2500 - 5s - loss: 0.8388 - accuracy: 0.6828\n",
      "Epoch 123/300\n",
      "2500/2500 - 5s - loss: 0.8396 - accuracy: 0.6830\n",
      "Epoch 124/300\n",
      "2500/2500 - 5s - loss: 0.8398 - accuracy: 0.6836\n",
      "Epoch 125/300\n",
      "2500/2500 - 5s - loss: 0.8385 - accuracy: 0.6828\n",
      "Epoch 126/300\n",
      "2500/2500 - 5s - loss: 0.8402 - accuracy: 0.6825\n",
      "Epoch 127/300\n",
      "2500/2500 - 5s - loss: 0.8376 - accuracy: 0.6829\n",
      "Epoch 128/300\n",
      "2500/2500 - 5s - loss: 0.8391 - accuracy: 0.6820\n",
      "Epoch 129/300\n",
      "2500/2500 - 5s - loss: 0.8387 - accuracy: 0.6825\n",
      "Epoch 130/300\n",
      "2500/2500 - 5s - loss: 0.8394 - accuracy: 0.6834\n",
      "Epoch 131/300\n",
      "2500/2500 - 5s - loss: 0.8365 - accuracy: 0.6844\n",
      "Epoch 132/300\n",
      "2500/2500 - 5s - loss: 0.8386 - accuracy: 0.6830\n",
      "Epoch 133/300\n",
      "2500/2500 - 5s - loss: 0.8376 - accuracy: 0.6838\n",
      "Epoch 134/300\n",
      "2500/2500 - 5s - loss: 0.8373 - accuracy: 0.6839\n",
      "Epoch 135/300\n",
      "2500/2500 - 5s - loss: 0.8387 - accuracy: 0.6841\n",
      "Epoch 136/300\n",
      "2500/2500 - 5s - loss: 0.8363 - accuracy: 0.6843\n",
      "Epoch 137/300\n",
      "2500/2500 - 5s - loss: 0.8366 - accuracy: 0.6848\n",
      "Epoch 138/300\n",
      "2500/2500 - 5s - loss: 0.8371 - accuracy: 0.6841\n",
      "Epoch 139/300\n",
      "2500/2500 - 5s - loss: 0.8352 - accuracy: 0.6839\n",
      "Epoch 140/300\n",
      "2500/2500 - 5s - loss: 0.8373 - accuracy: 0.6837\n",
      "Epoch 141/300\n",
      "2500/2500 - 5s - loss: 0.8361 - accuracy: 0.6833\n",
      "Epoch 142/300\n",
      "2500/2500 - 5s - loss: 0.8372 - accuracy: 0.6851\n",
      "Epoch 143/300\n",
      "2500/2500 - 5s - loss: 0.8367 - accuracy: 0.6820\n",
      "Epoch 144/300\n",
      "2500/2500 - 5s - loss: 0.8362 - accuracy: 0.6838\n",
      "Epoch 145/300\n",
      "2500/2500 - 5s - loss: 0.8338 - accuracy: 0.6849\n",
      "Epoch 146/300\n",
      "2500/2500 - 5s - loss: 0.8373 - accuracy: 0.6836\n",
      "Epoch 147/300\n",
      "2500/2500 - 5s - loss: 0.8347 - accuracy: 0.6844\n",
      "Epoch 148/300\n",
      "2500/2500 - 5s - loss: 0.8344 - accuracy: 0.6850\n",
      "Epoch 149/300\n",
      "2500/2500 - 5s - loss: 0.8346 - accuracy: 0.6843\n",
      "Epoch 150/300\n",
      "2500/2500 - 5s - loss: 0.8358 - accuracy: 0.6844\n",
      "Epoch 151/300\n",
      "2500/2500 - 5s - loss: 0.8339 - accuracy: 0.6835\n",
      "Epoch 152/300\n",
      "2500/2500 - 5s - loss: 0.8346 - accuracy: 0.6843\n",
      "Epoch 153/300\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_dim=300))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "# adam=keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss='CategoricalCrossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vecs_w2v, encoded, epochs=300, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_y_test = to_categorical(y_test)\n",
    "encoded_y_test\n",
    "score = model.evaluate(test_vecs_w2v, encoded_y_test, verbose=2)\n",
    "print(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# params={}\n",
    "# params['learning_rate']=0.03\n",
    "# params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "# params['objective']='multiclass' #Multi-class target feature\n",
    "# params['metric']='multi_logloss' #metric for multi-class\n",
    "# params['max_depth']=10\n",
    "# params['num_class']=5 #no.of unique values in the target class not inclusive of the end value\n",
    "\n",
    "# OLGBM =LGBMClassifier(**params)\n",
    "SLGBM =LGBMClassifier(n_estimators =1000,  n_jobs=-1,verbose=1)\n",
    "clf = make_pipeline(sc,SLGBM)\n",
    "clf.fit(train_vecs_w2v, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict_Result(y, predicted,predicted_proba):\n",
    "   \n",
    "\n",
    "\t\n",
    "\tconfusion = confusion_matrix(y, predicted)\n",
    "\n",
    "\tTP = confusion[1, 1]\n",
    "\tTN = confusion[0, 0]\n",
    "\tFP = confusion[0, 1]\n",
    "\tFN = confusion[1, 0]\n",
    "\n",
    "\n",
    "\t# Specificity\n",
    "\tSPE_cla = (TN/float(TN+FP))*100 \n",
    "\n",
    "\t# False Positive Rate\n",
    "\tFPR = (FP/float(TN+FP))\n",
    "\n",
    "\t#False Negative Rate (Miss Rate)\n",
    "\tFNR = (FN/float(FN+TP))\n",
    "\n",
    "\t#Balanced Accuracy\n",
    "\tACC_Bal = 0.5*((TP/float(TP+FN))+(TN/float(TN+FP)))*100\n",
    "\n",
    "\t# compute MCC\n",
    "\tMCC_cla = matthews_corrcoef(y, predicted)\n",
    "\tF1_cla = f1_score(y, predicted)\n",
    "\tPREC_cla = precision_score(y, predicted)*100\n",
    "\tREC_cla = recall_score(y, predicted)*100\n",
    "\tAccuracy_cla = accuracy_score(y, predicted)*100\n",
    "\n",
    "\n",
    "\tROC_auc_score= roc_auc_score(y, predicted_proba)*100\n",
    "\tPR_auc_score= average_precision_score(y, predicted_proba)*100\n",
    "\tsw = (REC_cla+SPE_cla-1)\n",
    "\n",
    "\n",
    "\n",
    "\tprint(f\"SN (%), {REC_cla:.2f}\")\n",
    "\tprint(f\"SP (%), {SPE_cla:.2f}\")\n",
    "\tprint(f\"Sw (%), {sw:.2f}\")\n",
    "\tprint(f\"BACC (%), {ACC_Bal:.2f}\")\n",
    "\tprint(f\"MCC, {MCC_cla:.3f}\")\n",
    "\tprint(f\"ACC (%), {Accuracy_cla:.2f}\") \n",
    "\tprint(f\"FPR, {FPR:.3f}\")\n",
    "\tprint(f\"FNR, {FNR:.3f}\")\n",
    "\tprint(f\"PR (%), {PREC_cla:.2f}\")\n",
    "\tprint(f\"F1-score, {F1_cla:.3f}\") \n",
    "\tprint(f\"ROCAUC (%), {ROC_auc_score:.2f}\")\n",
    "\tprint(f\"PRAUC (%), {PR_auc_score:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "predicted=clf.predict(test_vecs_w2v)\n",
    "predicted_proba=clf.predict_proba(test_vecs_w2v)\n",
    "# threshold=0.5\n",
    "# y_pred = (prob_predict0[:,1] >= threshold).astype(int)\n",
    "# Predict_Result(y_test.astype(float), np.array(y_pred),prob_predict0[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prob_predict0)\n",
    "y=y_test\n",
    "MCC_cla = matthews_corrcoef(y, predicted)\n",
    "F1_cla = f1_score(y, predicted,average=\"micro\")\n",
    "PREC_cla = precision_score(y, predicted,average=\"micro\")*100\n",
    "REC_cla = recall_score(y, predicted,average=\"micro\")*100\n",
    "Accuracy_cla = accuracy_score(y, predicted)*100\n",
    "ROC_auc_score= roc_auc_score(y, predicted_proba,multi_class=\"ovr\")*100\n",
    "\n",
    "print(f\"SN (%), {REC_cla:.2f}\")\n",
    "print(f\"MCC, {MCC_cla:.3f}\")\n",
    "print(f\"ACC (%), {Accuracy_cla:.2f}\") \n",
    "print(f\"PR (%), {PREC_cla:.2f}\")\n",
    "print(f\"F1-score, {F1_cla:.3f}\") \n",
    "print(f\"ROCAUC (%), {ROC_auc_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
