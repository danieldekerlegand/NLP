{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.preprocessing import scale\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99996, 2)\n",
      "(99996, 2)\n",
      "(99996, 2)\n",
      "(99996, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @Papapishu: Man it would fucking rule if we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>RT @Vitiligoprince: Hate Being sexually Frustr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                            context\n",
       "0         1                                                   \n",
       "1         2  RT @Papapishu: Man it would fucking rule if we...\n",
       "2         3                                                   \n",
       "3         4                                                   \n",
       "4         5                                                   \n",
       "5         6  RT @Vitiligoprince: Hate Being sexually Frustr..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @Papapishu: Man it would fucking rule if we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>RT @Vitiligoprince: Hate Being sexually Frustr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                            context\n",
       "0         1                                                   \n",
       "1         2  RT @Papapishu: Man it would fucking rule if we...\n",
       "2         3                                                   \n",
       "3         4                                                   \n",
       "4         5                                                   \n",
       "5         6  RT @Vitiligoprince: Hate Being sexually Frustr..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the twitter dataset from the file\n",
    "data = pd.read_csv('dataset.csv',index_col=False)\n",
    "print(data.shape)\n",
    "data.head(6)\n",
    "\n",
    "context_data = pd.read_csv('context.csv',index_col=False)\n",
    "context_data = context_data.replace(np.nan, '', regex=True)\n",
    "print(context_data.shape)\n",
    "context_data.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.tweet,data.label,test_size = 0.2)\n",
    "\n",
    "ContextX_train, ContextX_test = train_test_split(context_data.context,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78828    we are all affected by the destruction of our ...\n",
      "44043    shaktisinhgohil adani group s asset are perfor...\n",
      "83830    she s finishing a vodka lemonade i think he s ...\n",
      "Name: tweet, dtype: object\n",
      "(79996,)\n",
      "65291    as if jermainejunior hadn t played the victim ...\n",
      "71500    my child wont be have to ask when daddy coming...\n",
      "9053     you look like a keeper can i have your number ...\n",
      "Name: tweet, dtype: object\n",
      "(20000,)\n",
      "78828    we are all affected by the destruction of our ...\n",
      "44043    shaktisinhgohil adani group s asset are perfor...\n",
      "83830    she s finishing a vodka lemonade i think he s ...\n",
      "Name: tweet, dtype: object\n",
      "(79996,)\n",
      "65291    as if jermainejunior hadn t played the victim ...\n",
      "71500    my child wont be have to ask when daddy coming...\n",
      "9053     you look like a keeper can i have your number ...\n",
      "Name: tweet, dtype: object\n",
      "(20000,)\n",
      "3193     \n",
      "22599    \n",
      "53266    \n",
      "Name: context, dtype: object\n",
      "(79996,)\n",
      "41996    \n",
      "60170    \n",
      "34294    \n",
      "Name: context, dtype: object\n",
      "(20000,)\n",
      "3193     \n",
      "22599    \n",
      "53266    \n",
      "Name: context, dtype: object\n",
      "(79996,)\n",
      "41996    \n",
      "60170    \n",
      "34294    \n",
      "Name: context, dtype: object\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "# Clean the text by removing ...\n",
    "\n",
    "def to_lower(word):\n",
    "    result = word.lower()\n",
    "    return result\n",
    "\n",
    "def remove_hyperlink(word):\n",
    "    return  re.sub(r\"http\\S+\", \"\", word)\n",
    "\n",
    "def remove_mentions(word):\n",
    "    return re.sub(r\"@\\S+\", \"\", word)\n",
    "\n",
    "def remove_number(word):\n",
    "    result = re.sub(r'\\d+', '', word)\n",
    "    return result\n",
    "\n",
    "def remove_punctuation(word):\n",
    "    result = re.sub('[^A-Za-z]+', ' ', word)\n",
    "    return result\n",
    "\n",
    "def remove_whitespace(word):\n",
    "    result = word.strip()\n",
    "    return result\n",
    "\n",
    "def replace_newline(word):\n",
    "    return word.replace('\\n','')\n",
    "\n",
    "def remove_stopwords(word):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    return ' '.join(word for word in i.split() if word not in stopwords)\n",
    "\n",
    "def clean_up_pipeline(sentence):\n",
    "    cleaning_data = [remove_hyperlink,\n",
    "                      replace_newline,\n",
    "                      to_lower,\n",
    "                      remove_number,\n",
    "                      remove_punctuation,\n",
    "                      remove_whitespace]\n",
    "    for func in cleaning_data:\n",
    "        \n",
    "        sentence = func(sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "X_train = X_train.apply(clean_up_pipeline)\n",
    "X_test = X_test.apply(clean_up_pipeline)\n",
    "print(X_train[:3])\n",
    "print(X_train.shape)\n",
    "print(X_test[:3])\n",
    "print(X_test.shape)\n",
    "\n",
    "ContextX_train = ContextX_train.apply(clean_up_pipeline)\n",
    "ContextX_test = ContextX_test.apply(clean_up_pipeline)\n",
    "print(ContextX_train[:3])\n",
    "print(ContextX_train.shape)\n",
    "print(ContextX_test[:3])\n",
    "print(ContextX_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78828    we are all affected by the destruction of our ...\n",
      "44043    shaktisinhgohil adani group s asset are perfor...\n",
      "83830    she s finishing a vodka lemonade i think he s ...\n",
      "Name: tweet, dtype: object\n",
      "(79996,)\n",
      "65291    as if jermainejunior hadn t played the victim ...\n",
      "71500    my child wont be have to ask when daddy coming...\n",
      "9053     you look like a keeper can i have your number ...\n",
      "Name: tweet, dtype: object\n",
      "(20000,)\n",
      "78828    we are all affected by the destruction of our ...\n",
      "44043    shaktisinhgohil adani group s asset are perfor...\n",
      "83830    she s finishing a vodka lemonade i think he s ...\n",
      "Name: tweet, dtype: object\n",
      "(79996,)\n",
      "65291    as if jermainejunior hadn t played the victim ...\n",
      "71500    my child wont be have to ask when daddy coming...\n",
      "9053     you look like a keeper can i have your number ...\n",
      "Name: tweet, dtype: object\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "# X_train = X_train.apply(word_tokenize)\n",
    "# X_test = X_test.apply(word_tokenize)\n",
    "print(X_train[:3])\n",
    "print(X_train.shape)\n",
    "print(X_test[:3])\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text])\n",
    "\n",
    "X_train = X_train.apply(lambda text: stem_words(text))\n",
    "X_test = X_test.apply(lambda text: stem_words(text))\n",
    "\n",
    "ContextX_train = ContextX_train.apply(lambda text: stem_words(text))\n",
    "ContextX_test = ContextX_test.apply(lambda text: stem_words(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models.word2vec import Word2Vec \n",
    "# #Word Embedding using Word2Vec\n",
    "# wordvector = Word2Vec(vector_size=300, min_count=10)\n",
    "# wordvector.build_vocab([x for x in X_train])\n",
    "# wordvector.train([x for x in X_train],total_examples=wordvector.corpus_count, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordvector.wv['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "# # Load vectors directly from the file\n",
    "# model = KeyedVectors.load_word2vec_format('data/GoogleGoogleNews-vectors-negative300.bin', binary=True)\n",
    "# # Access vectors for specific words with a keyed lookup:\n",
    "# vector = model['easy']\n",
    "# # see the shape of the vector (300,)\n",
    "# vector.shape\n",
    "# # Processing sentences is not as simple as with Spacy:\n",
    "# vectors = [model[x] for x in \"This is some text I am processing with Spacy\".split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "# path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "# print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "glove_vectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_vectors['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += glove_vectors[word].reshape((1, size)) #* tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, 300) for z in tqdm(map(lambda x: x, X_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, 300) for z in map(lambda x: x, X_test)])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "context_train_vecs_w2v = np.concatenate([buildWordVector(z, 300) for z in tqdm(map(lambda x: x, ContextX_train))])\n",
    "context_train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "context_test_vecs_w2v = np.concatenate([buildWordVector(z, 300) for z in map(lambda x: x, ContextX_test)])\n",
    "context_test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)\n",
    "from keras.utils import to_categorical\n",
    "encoded = to_categorical(y_train)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Convolution1D, MaxPooling1D\n",
    "from keras.layers import Flatten, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class WordCNN_Ctxt(object):\n",
    "    def __init__(\n",
    "            self, sequence_length, n_classes, vocab_size,\n",
    "            filter_sizes, num_filters, learning_rate=0.001, \n",
    "            embedding_size=300, embedding_matrix=None, train_embedding=True):\n",
    "\n",
    "        inputs = Input(shape=(sequence_length,))\n",
    "        ctxt_inputs = Input(shape=(sequence_length,))\n",
    "\n",
    "        embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_size,\n",
    "                                    trainable=train_embedding, weights=[embedding_matrix])(inputs)\n",
    "        ctxt_embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_size,\n",
    "                                    trainable=train_embedding, weights=[embedding_matrix])(ctxt_inputs)\n",
    "\n",
    "        conv_blocks = []\n",
    "        for filter_size in filter_sizes:\n",
    "            conv = Convolution1D(filters=num_filters, kernel_size=filter_size,\n",
    "                                 padding=\"valid\", activation=\"relu\", strides=1)(embedding_layer)\n",
    "            conv = MaxPooling1D(pool_size=sequence_length - filter_size + 1)(conv)\n",
    "            conv = Flatten()(conv)\n",
    "            conv_blocks.append(conv)\n",
    "\n",
    "        ctxt_conv_blocks = []\n",
    "        for filter_size in filter_sizes:\n",
    "            ctxt_conv = Convolution1D(filters=num_filters, kernel_size=filter_size,\n",
    "                                 padding=\"valid\", activation=\"relu\", strides=1)(ctxt_embedding_layer)\n",
    "            ctxt_conv = MaxPooling1D(pool_size=sequence_length - filter_size + 1)(ctxt_conv)\n",
    "            ctxt_conv = Flatten()(ctxt_conv)\n",
    "            ctxt_conv_blocks.append(ctxt_conv)\n",
    "\n",
    "        cnn = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "        ctxt_cnn = Concatenate()(ctxt_conv_blocks) if len(ctxt_conv_blocks) > 1 else ctxt_conv_blocks[0]\n",
    "\n",
    "        cnn = Dense(9, activation=\"relu\")(cnn)\n",
    "        ctxt_cnn = Dense(6, activation=\"relu\")(ctxt_cnn) # Reduce dimension for context inputs.\n",
    "\n",
    "        merged_net = Concatenate()([ctxt_cnn, cnn])\n",
    "\n",
    "        output = Dense(n_classes, activation=\"softmax\")(merged_net)\n",
    "\n",
    "        self.model = Model([inputs,ctxt_inputs], output)\n",
    "\n",
    "        self.model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=learning_rate), metrics=[\"accuracy\"])\n",
    "        self.model.summary()\n",
    "        \n",
    "sequence_length = train_vecs_w2v.shape[1]\n",
    "n_classes = 4\n",
    "vocab_size = 3000000\n",
    "filter_sizes = [3]\n",
    "num_filters = 32\n",
    "        \n",
    "model = WordCNN_Ctxt(sequence_length=sequence_length, n_classes=n_classes, vocab_size=vocab_size, \n",
    "filter_sizes=filter_sizes, num_filters=num_filters, embedding_size=300)\n",
    "\n",
    "model.model.fit(x=[train_vecs_w2v, context_train_vecs_w2v], y=encoded, batch_size=32, verbose=2, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_dim=300))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "# adam=keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss='CategoricalCrossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vecs_w2v, encoded, epochs=300, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_y_test = to_categorical(y_test)\n",
    "encoded_y_test\n",
    "score = model.evaluate(test_vecs_w2v, encoded_y_test, verbose=2)\n",
    "print(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# params={}\n",
    "# params['learning_rate']=0.03\n",
    "# params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "# params['objective']='multiclass' #Multi-class target feature\n",
    "# params['metric']='multi_logloss' #metric for multi-class\n",
    "# params['max_depth']=10\n",
    "# params['num_class']=5 #no.of unique values in the target class not inclusive of the end value\n",
    "\n",
    "# OLGBM =LGBMClassifier(**params)\n",
    "SLGBM =LGBMClassifier(n_estimators =1000,  n_jobs=-1,verbose=1)\n",
    "clf = make_pipeline(sc,SLGBM)\n",
    "clf.fit(train_vecs_w2v, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict_Result(y, predicted,predicted_proba):\n",
    "    \n",
    "\tconfusion = confusion_matrix(y, predicted)\n",
    "\n",
    "\tTP = confusion[1, 1]\n",
    "\tTN = confusion[0, 0]\n",
    "\tFP = confusion[0, 1]\n",
    "\tFN = confusion[1, 0]\n",
    "\n",
    "\t# Specificity\n",
    "\tSPE_cla = (TN/float(TN+FP))*100 \n",
    "\n",
    "\t# False Positive Rate\n",
    "\tFPR = (FP/float(TN+FP))\n",
    "\n",
    "\t#False Negative Rate (Miss Rate)\n",
    "\tFNR = (FN/float(FN+TP))\n",
    "\n",
    "\t#Balanced Accuracy\n",
    "\tACC_Bal = 0.5*((TP/float(TP+FN))+(TN/float(TN+FP)))*100\n",
    "\n",
    "\t# compute MCC\n",
    "\tMCC_cla = matthews_corrcoef(y, predicted)\n",
    "\tF1_cla = f1_score(y, predicted)\n",
    "\tPREC_cla = precision_score(y, predicted)*100\n",
    "\tREC_cla = recall_score(y, predicted)*100\n",
    "\tAccuracy_cla = accuracy_score(y, predicted)*100\n",
    "\n",
    "\tROC_auc_score= roc_auc_score(y, predicted_proba)*100\n",
    "\tPR_auc_score= average_precision_score(y, predicted_proba)*100\n",
    "\tsw = (REC_cla+SPE_cla-1)\n",
    "\n",
    "\tprint(f\"SN (%), {REC_cla:.2f}\")\n",
    "\tprint(f\"SP (%), {SPE_cla:.2f}\")\n",
    "\tprint(f\"Sw (%), {sw:.2f}\")\n",
    "\tprint(f\"BACC (%), {ACC_Bal:.2f}\")\n",
    "\tprint(f\"MCC, {MCC_cla:.3f}\")\n",
    "\tprint(f\"ACC (%), {Accuracy_cla:.2f}\") \n",
    "\tprint(f\"FPR, {FPR:.3f}\")\n",
    "\tprint(f\"FNR, {FNR:.3f}\")\n",
    "\tprint(f\"PR (%), {PREC_cla:.2f}\")\n",
    "\tprint(f\"F1-score, {F1_cla:.3f}\") \n",
    "\tprint(f\"ROCAUC (%), {ROC_auc_score:.2f}\")\n",
    "\tprint(f\"PRAUC (%), {PR_auc_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "predicted=clf.predict(test_vecs_w2v)\n",
    "predicted_proba=clf.predict_proba(test_vecs_w2v)\n",
    "# threshold=0.5\n",
    "# y_pred = (prob_predict0[:,1] >= threshold).astype(int)\n",
    "# Predict_Result(y_test.astype(float), np.array(y_pred),prob_predict0[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prob_predict0)\n",
    "y=y_test\n",
    "MCC_cla = matthews_corrcoef(y, predicted)\n",
    "F1_cla = f1_score(y, predicted,average=\"micro\")\n",
    "PREC_cla = precision_score(y, predicted,average=\"micro\")*100\n",
    "REC_cla = recall_score(y, predicted,average=\"micro\")*100\n",
    "Accuracy_cla = accuracy_score(y, predicted)*100\n",
    "ROC_auc_score= roc_auc_score(y, predicted_proba,multi_class=\"ovr\")*100\n",
    "\n",
    "print(f\"SN (%), {REC_cla:.2f}\")\n",
    "print(f\"MCC, {MCC_cla:.3f}\")\n",
    "print(f\"ACC (%), {Accuracy_cla:.2f}\") \n",
    "print(f\"PR (%), {PREC_cla:.2f}\")\n",
    "print(f\"F1-score, {F1_cla:.3f}\") \n",
    "print(f\"ROCAUC (%), {ROC_auc_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
